---
title: "COD Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

#### Article ID: jCWSIW
#### Pilot: Vivian Xiao
#### Start date: 03/12/17
#### End date: 03/12/17

-------

#### Methods summary: 
A total of 202 students at at university were recruited to participate in the experiment. 2 participants were ultimately excluded because of failure to complete the study, making for a total sample of 200 university students. 

Participants were randomly assigned to one of two experimental conditions in a between-subjects design: standard typing condition vs. keyboard delay condition. Participants were given 50 minutes to write an essay about cellphone use in schools, and these essays were meant to be argumentative in nature, at least 500 words long, and participants were told that their essays would be graded. After a 3 minute practice session to get familiarized with the keyboard by typing a sentence, participants wrote their essay. After the 50 minutes ended, participants filled out NASA Task Load Index (NASA-TLX), which measures subjective fluency. 

The keyboard manipulation was conducted using a QWERTY keyboard and a software program that manipulated the delay between keystrokes. Following pilot tests, the minimum delay that was set using this software was 100ms. The program, as well as Inputlog key-logger (another program), recorded keystroke activity.

The times between consecutive lower case letters, as recorded by the keyboard delay program, was used as a measure of transcription fluency. Outliers (more than 2.5 SD away from the mean) were removed within individual participants - 1.42% of keystrokes were eliminated for 199 participants (one participant typed the essay in all caps). Using the Coh-Metrix text analyzer, essays were analyzed for lexical diversity and word frequency. The Tool for the Automatic Analysis of Lexical Sophistication (TAALES) and the Tool for the Automatic Analysis of Cohesion (TAACO) were also used. 

Lexical diversity was defined as the range of vocabulary in a passage of text. Specifically, this means the ratio of unique words to the number of words in a passage of text. A type-token ration (TTR), a measure of textual lexical diversity (MTLD), and vocd-D are calculated using the Coh-Metrix to operationalize this construct. TTR was also calculated using tbe TAACO measure. Word frequency is defined as how often an individual word occurs in English, with the assumption that passages of text that contain words that are infrequent are more lexically sophisticated. The Coh-Metrix uses the CELEX database to calculate the log-frequency for all words used as well as the raw frequencies for content words. SUBTLEXus and the British National Corpus databases are used by TAALES to assess word frequency as well. Further, three independent raters who were blind to condition scored the essays using a 6 point scale similar to that used by the ACT writing test. All raters were trained using the same rubric and a pool of argumentative essays separate from this study. Average interrater reliability reached r > .5 on these training trials, and when the essays from this study were scored, the interrater reliability was adequate, r(198) = .56, p < .001. The mean of the two raters' scores with the highest correlation were used as the ultimate score assigned to each essay. The score closest to the third rater's score was used in the event that the differences between the two raters were greater than or equal to 1.

------

#### Target outcomes: 
> A series of one-way ANOVAs were performed with condition (standard vs. keyboard delay) as the factor and transcription fluency, lexical sophistication, subjective fluency, and essay quality as the dependent variables.

> 3.1. Descriptive essay indices

> Condition affected transcription fluency such that it was more fluent in the standard condition, F(1, 197) = 110.14, MSE = 2796.13, p < 0.001, d = 1.50. While there were more words typed in the standard condition, F(1, 198) = 6.35, MSE = 14329.49, p = 0.013, d = 0.36, essays in the delay condition contained longer words (letters and syllables per word), Fs > 4.17, ps < 0.043, ds > 0.28. There were no other differences (see Table 1).

> 3.2. Lexical sophistication

> Condition had a significant effect on type-token ratio, F(1, 198) = 9.70, MSE = 0.002, p = 0.002, d = 0.44, and vocd-D (marginally), F(1, 198) = 3.29, MSE = 264.62, p = 0.071, d = 0.26, such that they were higher in the delay condition. There was no effect on the measure of textual lexical diversity, F(1, 198) = 2.13, MSE = 239.96, p = 0.146, d = 0.21, though the pattern of means was in the same direction. Moreover, condition had an effect on both word frequency indices such that they were lower in the delay condition, log word frequency-all words, F(1, 198) = 4.49, MSE = 0.01, p = 0.035, d = 0.30, raw word frequency-content words, F(1, 198) = 4.74, MSE = 0.02, p = 0.031, d = 0.31 (see Table 2). The results were similar when lexical diversity and word frequency from TAACO and TAALES were used (see Table 3) (from Medimorec et al., p. 30).

------


```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

## Step 1: Load packages

```{r}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CODreports) # custom report functions
library(lsr)
library(tables)
```

## Step 2: Load data

First, we load the datasets necessary to carry out this reproduction of results.
```{r}
d1 = read.csv("~/Documents/set_jCSIW/data/data1.csv", stringsAsFactors = F)
d2 = read.csv("~/Documents/set_jCSIW/data/data2.csv", stringsAsFactors = F)
d3 = read.csv("~/Documents/set_jCSIW/data/data3.csv", stringsAsFactors = F)
```

## Step 3: Tidy data

Next, we join the two datasets to recreate the full dataset that was likely used in the original analyses.
```{r}
d.tidy = left_join(d1, d2)
```

## Step 4: Run analysis

### Descriptive statistics

Here, we calculate the means and standard deviations of each of the recorded DV's broken down by condition in order to attempt to recreate Medimorec et al.'s reported table of means and standard deviations.
```{r}
#table 1
d.tidy$Transcription_Fluency = as.numeric(d.tidy$Transcription_Fluency)
m_fluency = mean(d.tidy$Transcription_Fluency[d.tidy$Condition_0.Standard == "0"], na.rm=T)
sd_fluency = sd(d.tidy$Transcription_Fluency[d.tidy$Condition_0.Standard == "0"], na.rm = T)
m_fluency2 = mean(d.tidy$Transcription_Fluency[d.tidy$Condition_0.Standard == "1"], na.rm = T)
sd_fluency2 = sd(d.tidy$Transcription_Fluency[d.tidy$Condition_0.Standard == "1"], na.rm = T)

m_numwords = mean(d.tidy$Number_of_Words[d.tidy$Condition_0.Standard == "0"], na.rm=T)
sd_numwords = sd(d.tidy$Number_of_Words[d.tidy$Condition_0.Standard == "0"], na.rm = T)
m_numwords2 = mean(d.tidy$Number_of_Words[d.tidy$Condition_0.Standard == "1"], na.rm = T)
sd_numwords2 = sd(d.tidy$Number_of_Words[d.tidy$Condition_0.Standard == "1"], na.rm = T)

m_lengthl = mean(d.tidy$Letters_Per_Word[d.tidy$Condition_0.Standard == "0"], na.rm=T)
sd_lengthl = sd(d.tidy$Letters_Per_Word[d.tidy$Condition_0.Standard == "0"], na.rm = T)
m_lengthl2 = mean(d.tidy$Letters_Per_Word[d.tidy$Condition_0.Standard == "1"], na.rm = T)
sd_lengthl2 = sd(d.tidy$Letters_Per_Word[d.tidy$Condition_0.Standard == "1"], na.rm = T)

m_lengths = mean(d.tidy$Syllables_Per_Word[d.tidy$Condition_0.Standard == "0"], na.rm=T)
sd_lengths = sd(d.tidy$Syllables_Per_Word[d.tidy$Condition_0.Standard == "0"], na.rm = T)
m_lengths2 = mean(d.tidy$Syllables_Per_Word[d.tidy$Condition_0.Standard == "1"], na.rm = T)
sd_lengths2 = sd(d.tidy$Syllables_Per_Word[d.tidy$Condition_0.Standard == "1"], na.rm = T)

m_sentence = mean(d.tidy$Sentence_count[d.tidy$Condition_0.Standard == "0"], na.rm=T)
sd_sentence = sd(d.tidy$Sentence_count[d.tidy$Condition_0.Standard == "0"], na.rm = T)
m_sentence2 = mean(d.tidy$Sentence_count[d.tidy$Condition_0.Standard == "1"], na.rm = T)
sd_sentence2 = sd(d.tidy$Sentence_count[d.tidy$Condition_0.Standard == "1"], na.rm = T)

m_wordsper = mean(d.tidy$words_per_sentence[d.tidy$Condition_0.Standard == "0"], na.rm=T)
sd_wordsper = sd(d.tidy$words_per_sentence[d.tidy$Condition_0.Standard == "0"], na.rm = T)
m_wordsper2 = mean(d.tidy$words_per_sentence[d.tidy$Condition_0.Standard == "1"], na.rm = T)
sd_wordsper2 = sd(d.tidy$words_per_sentence[d.tidy$Condition_0.Standard == "1"], na.rm = T)

m_paragraph = mean(d.tidy$paragraph_count[d.tidy$Condition_0.Standard == "0"], na.rm=T)
sd_paragraph = sd(d.tidy$paragraph_count[d.tidy$Condition_0.Standard == "0"], na.rm = T)
m_paragraph2 = mean(d.tidy$paragraph_count[d.tidy$Condition_0.Standard == "1"], na.rm = T)
sd_paragraph2 = sd(d.tidy$paragraph_count[d.tidy$Condition_0.Standard == "1"], na.rm = T)

t1 = as.table(matrix(c(m_fluency,sd_fluency,m_fluency2,sd_fluency2,m_numwords,sd_numwords,m_numwords2,sd_numwords2,m_lengthl, sd_lengthl,m_lengthl2,sd_lengthl2,m_lengths,sd_lengths,m_lengths2,sd_lengths2,m_sentence,sd_sentence, m_sentence2,sd_sentence2, m_wordsper, sd_wordsper,m_wordsper2,sd_wordsper2,m_paragraph,sd_paragraph,m_paragraph2, sd_paragraph2), byrow=T, ncol=4,
              dimnames=list(measure=c("transcription fluency","number of words", "word length (letters)", "world length (syllables)", "sentence count", "words per sentence", "paragraph count"),
                            statistic=c("m (standard)", "sd (standard)", "m (delay)", "sd (delay)"))))
print(t1)

#table 2
m_ttr = mean(d.tidy$Type.Token_Ratio[d.tidy$Condition_0.Standard == "0"])
sd_ttr = sd(d.tidy$Type.Token_Ratio[d.tidy$Condition_0.Standard == "0"])
m_ttr2 = mean(d.tidy$Type.Token_Ratio[d.tidy$Condition_0.Standard == "1"])
sd_ttr2 = sd(d.tidy$Type.Token_Ratio[d.tidy$Condition_0.Standard == "1"])

m_mtld = mean(d.tidy$Measure_of_Textual_Lexical_Diversity_MTLD[d.tidy$Condition_0.Standard == "0"])
sd_mtld = sd(d.tidy$Measure_of_Textual_Lexical_Diversity_MTLD[d.tidy$Condition_0.Standard == "0"])
m_mtld2 = mean(d.tidy$Measure_of_Textual_Lexical_Diversity_MTLD[d.tidy$Condition_0.Standard == "1"])
sd_mtld2 = sd(d.tidy$Measure_of_Textual_Lexical_Diversity_MTLD[d.tidy$Condition_0.Standard == "1"])

m_vocd = mean(d.tidy$vocdD_Lexical_Diversity[d.tidy$Condition_0.Standard == "0"])
sd_vocd = sd(d.tidy$vocdD_Lexical_Diversity[d.tidy$Condition_0.Standard == "0"])
m_vocd2 = mean(d.tidy$vocdD_Lexical_Diversity[d.tidy$Condition_0.Standard == "1"])
sd_vocd2 = sd(d.tidy$vocdD_Lexical_Diversity[d.tidy$Condition_0.Standard == "1"])

m_logfreq = mean(d.tidy$Word_Frequency_all_words_log[d.tidy$Condition_0.Standard == "0"])
sd_logfreq = sd(d.tidy$Word_Frequency_all_words_log[d.tidy$Condition_0.Standard == "0"])
m_logfreq2 = mean(d.tidy$Word_Frequency_all_words_log[d.tidy$Condition_0.Standard == "1"])
sd_logfreq2 = sd(d.tidy$Word_Frequency_all_words_log[d.tidy$Condition_0.Standard == "1"])

m_rawfreq = mean(d.tidy$Word_Frequency_content_words_raw[d.tidy$Condition_0.Standard == "0"])
sd_rawfreq = sd(d.tidy$Word_Frequency_content_words_raw[d.tidy$Condition_0.Standard == "0"])
m_rawfreq2 = mean(d.tidy$Word_Frequency_content_words_raw[d.tidy$Condition_0.Standard == "1"])
sd_rawfreq2 = sd(d.tidy$Word_Frequency_content_words_raw[d.tidy$Condition_0.Standard == "1"])

t2 = as.table(matrix(c(m_ttr, sd_ttr, m_ttr2, sd_ttr2, m_mtld, sd_mtld, m_mtld2, sd_mtld2,m_vocd,sd_vocd,m_vocd2,sd_vocd2,m_logfreq,sd_logfreq,m_logfreq2, sd_logfreq2, m_rawfreq,sd_rawfreq, m_rawfreq2,sd_rawfreq2), byrow=T, ncol=4,
              dimnames=list(measure=c("type-token ratio", "measure of textual lexical diversity","vocd-D", "log frequency all words", "word frequency content words (raw)"),
                            statistic=c("m (standard)", "sd (standard)", "m (delay)", "sd (delay)"))))
print(t2)


#table 3
m_ttr3 = mean(d.tidy$type.token_ratio_TAACO[d.tidy$Condition_0.Standard == "0"])
sd_ttr3 = sd(d.tidy$type.token_ratio_TAACO[d.tidy$Condition_0.Standard == "0"])
m_ttr4 = mean(d.tidy$type.token_ratio_TAACO[d.tidy$Condition_0.Standard == "1"])
sd_ttr4 = sd(d.tidy$type.token_ratio_TAACO[d.tidy$Condition_0.Standard == "1"])

m_allsub = mean(d.tidy$SUBTLEXusFreq_ALL_Words_Log[d.tidy$Condition_0.Standard == "0"])
sd_allsub = sd(d.tidy$SUBTLEXusFreq_ALL_Words_Log[d.tidy$Condition_0.Standard == "0"])
m_allsub2 = mean(d.tidy$SUBTLEXusFreq_ALL_Words_Log[d.tidy$Condition_0.Standard == "1"])
sd_allsub2 = sd(d.tidy$SUBTLEXusFreq_ALL_Words_Log[d.tidy$Condition_0.Standard == "1"])

m_allbnc = mean(d.tidy$BNCWrittenFreq_AllWords_Log[d.tidy$Condition_0.Standard == "0"])
sd_allbnc = sd(d.tidy$BNCWrittenFreq_AllWords_Log[d.tidy$Condition_0.Standard == "0"])
m_allbnc2 = mean(d.tidy$BNCWrittenFreq_AllWords_Log[d.tidy$Condition_0.Standard == "1"])
sd_allbnc2 = sd(d.tidy$BNCWrittenFreq_AllWords_Log[d.tidy$Condition_0.Standard == "1"])

m_contentsub = mean(d.tidy$SUBTLEXusFreq_Content_Words_Log[d.tidy$Condition_0.Standard == "0"])
sd_contentsub = sd(d.tidy$SUBTLEXusFreq_Content_Words_Log[d.tidy$Condition_0.Standard == "0"])
m_contentsub2 = mean(d.tidy$SUBTLEXusFreq_Content_Words_Log[d.tidy$Condition_0.Standard == "1"])
sd_contentsub2 = sd(d.tidy$SUBTLEXusFreq_Content_Words_Log[d.tidy$Condition_0.Standard == "1"])

m_contentbnc = mean(d.tidy$BNCWrittenFreq_Content_Words_Log[d.tidy$Condition_0.Standard == "0"])
sd_contentbnc = sd(d.tidy$BNCWrittenFreq_Content_Words_Log[d.tidy$Condition_0.Standard == "0"])
m_contentbnc2 = mean(d.tidy$BNCWrittenFreq_Content_Words_Log[d.tidy$Condition_0.Standard == "1"])
sd_contentbnc2 = sd(d.tidy$BNCWrittenFreq_Content_Words_Log[d.tidy$Condition_0.Standard == "1"])

t3 = as.table(matrix(c(m_ttr3, sd_ttr3, m_ttr4, sd_ttr4, m_allsub, sd_allsub, m_allsub2, sd_allsub2, m_allbnc, sd_allbnc, m_allbnc2,sd_allbnc2,m_contentsub, sd_contentsub, m_contentsub2, sd_contentsub2, m_contentbnc, sd_contentbnc, m_contentbnc2, sd_contentbnc2), byrow=T, ncol=4,
              dimnames=list(measure=c("type-token ratio", "log frequency all words(SUBTLEXus)", "log frequency all words (BNC)", "log frequency content words (SUBTLEXus)", "log frequency content words (BNC)"),
                            statistic=c("m (standard)", "sd (standard)", "m (delay)", "sd (delay)"))))
print(t3)

```

### Inferential statistics

Now we move onto reproducing Medimorec et al.'s main analyses. We first attempt to reproduce their analyses of descriptive essay indices, noting where there are discrepancies between our obtained results and Medimorec et al.'s reported results.
```{r}
#3.1 Descriptive essay indices
rs1 = summary(lm(Transcription_Fluency ~ Condition_0.Standard, d.tidy)); rs1
effect = cohensD(Transcription_Fluency ~ Condition_0.Standard, data = d.tidy); effect
compareValues(reportedValue = 1.50, obtainedValue = 1.49, isP = F) #MINOR ERROR
compareValues(reportedValue = 2796.13, obtainedValue = mean(52.88^2), isP = F) #MINOR ERROR

rs2 = summary(lm(Number_of_Words ~ Condition_0.Standard, d.tidy)); rs2
effect2 = cohensD(Number_of_Words ~ Condition_0.Standard, data = d.tidy); effect2
compareValues(reportedValue = 14329.49, obtainedValue = mean(119.7^2), isP = F) #MINOR ERROR

rs3 = summary(lm(Letters_Per_Word ~ Condition_0.Standard, d.tidy)); rs3
effect3 = cohensD(Letters_Per_Word ~ Condition_0.Standard, data = d.tidy); effect3
compareValues(reportedValue = .043, obtainedValue = .041, isP = T) #MINOR ERROR
compareValues(reportedValue = 4.17, obtainedValue = 2.059^2, isP = F) #MINOR ERROR
compareValues(reportedValue = .28, obtainedValue = .29, isP = F) #MINOR ERROR

rs4 = summary(lm(Syllables_Per_Word ~ Condition_0.Standard, d.tidy)); rs4
effect4 = cohensD(Syllables_Per_Word ~ Condition_0.Standard, data = d.tidy); effect4
#INSUFFICIENT INFO

rs5 = summary(lm(Sentence_count ~ Condition_0.Standard, d.tidy)); rs5
effect5 = cohensD(Sentence_count ~ Condition_0.Standard, data = d.tidy); effect5
#INSUFFICIENT INFO

rs6 = summary(lm(words_per_sentence ~ Condition_0.Standard, d.tidy)); rs6
effect6 = cohensD(words_per_sentence ~ Condition_0.Standard, data = d.tidy); effect6
#INSUFFICIENT INFO

rs7 = summary(lm(paragraph_count ~ Condition_0.Standard, d.tidy)); rs7
effect7 = cohensD(paragraph_count ~ Condition_0.Standard, data = d.tidy); effect7
#INSUFFICIENT INFO
```

Next, we reproduce Medimorac et al.'s analyses of lexical sophistication, using the Coh-Metrix source as well as the TAACO and TAALES sources, again noting where there are discrepancies between our obtained results and Medimorec et al.'s reported results.
```{r}
#3.2 Lexical sophistication
rs8 = summary(lm(Type.Token_Ratio ~ Condition_0.Standard, d.tidy)); rs8
effect8 = cohensD(Type.Token_Ratio ~ Condition_0.Standard, data = d.tidy); effect8

rs9 = summary(lm(vocdD_Lexical_Diversity ~ Condition_0.Standard, d.tidy)); rs9
effect9 = cohensD(vocdD_Lexical_Diversity ~ Condition_0.Standard, data = d.tidy); effect9
compareValues(reportedValue = 3.29, obtainedValue = 3.28, isP = F) #MINOR ERROR
compareValues(reportedValue = 264.62, obtainedValue = mean(16.27^2), isP = F) #MINOR ERROR

rs10 = summary(lm(Measure_of_Textual_Lexical_Diversity_MTLD ~ Condition_0.Standard, d.tidy)); rs10
effect10 = cohensD(Measure_of_Textual_Lexical_Diversity_MTLD ~ Condition_0.Standard, data = d.tidy); effect10
compareValues(reportedValue = 239.96, obtainedValue = mean(15.49^2), isP = F) #MINOR ERROR

rs11 = summary(lm(Word_Frequency_all_words_log ~ Condition_0.Standard, d.tidy)); rs11
effect11 = cohensD(Word_Frequency_all_words_log ~ Condition_0.Standard, data = d.tidy); effect11
compareValues(reportedValue = .01, obtainedValue = mean(0.08878^2), isP = F) #MAJOR ERROR

rs12 = summary(lm(Word_Frequency_content_words_raw ~ Condition_0.Standard, d.tidy)); rs12
effect12 = cohensD(Word_Frequency_content_words_raw ~ Condition_0.Standard, data = d.tidy); effect12
compareValues(reportedValue = 4.74, obtainedValue = (-2.176)^2, isP = F) #MINOR ERROR

#TAACO/TAALES
rs13 = summary(lm(type.token_ratio_TAACO ~ Condition_0.Standard, d.tidy)); rs13
effect13 = cohensD(type.token_ratio_TAACO ~ Condition_0.Standard, data = d.tidy); effect13
#INSUFFICIENT INFO

rs14 = summary(lm(SUBTLEXusFreq_ALL_Words_Log ~ Condition_0.Standard, d.tidy)); rs14
effect14 = cohensD(SUBTLEXusFreq_ALL_Words_Log ~ Condition_0.Standard, data = d.tidy); effect14
#INSUFFICIENT INFO

rs15 = summary(lm(BNCWrittenFreq_AllWords_Log ~ Condition_0.Standard, d.tidy)); rs15
effect15 = cohensD(BNCWrittenFreq_AllWords_Log ~ Condition_0.Standard, data = d.tidy); effect15
#INSUFFICIENT INFO

rs16 = summary(lm(SUBTLEXusFreq_Content_Words_Log ~ Condition_0.Standard, d.tidy)); rs16
effect16 = cohensD(SUBTLEXusFreq_Content_Words_Log ~ Condition_0.Standard, data = d.tidy); effect16
#INSUFFICIENT INFO

rs17 = summary(lm(BNCWrittenFreq_Content_Words_Log ~ Condition_0.Standard, d.tidy)); rs17
effect17 = cohensD(BNCWrittenFreq_Content_Words_Log ~ Condition_0.Standard, data = d.tidy); effect17
#INSUFFICIENT INFO
```


## Step 5: Conclusion

Here, we summarize the errors that we encountered in our reproduction of our analyses. 
```{r-report errors}
codReport(Report_Type = 'pilot',
          Article_ID = 'jCWSIW', 
          Insufficient_Information_Errors = 9,
          Decision_Errors = 0, 
          Major_Numerical_Errors = 1, 
          Minor_Numerical_Errors = 10)
```

Below, we reconstruct the tables that Medimorec et al. reported in their original paper. 

**Table 1**: means, standard deviations, and Cohen's d of essay discriptive measures across the two conditions.
```{r}
t1 = as.table(matrix(c(m_fluency,sd_fluency,m_fluency2,sd_fluency2,effect, m_numwords,sd_numwords,m_numwords2,sd_numwords2,effect2, m_lengthl, sd_lengthl,m_lengthl2,sd_lengthl2,effect3, m_lengths,sd_lengths,m_lengths2,sd_lengths2,effect4,m_sentence,sd_sentence, m_sentence2,sd_sentence2, effect5, m_wordsper, sd_wordsper,m_wordsper2,sd_wordsper2,effect6,m_paragraph,sd_paragraph,m_paragraph2, sd_paragraph2, effect7), byrow=T, ncol=5,
              dimnames=list(measure=c("transcription fluency","number of words", "word length (letters)", "world length (syllables)", "sentence count", "words per sentence", "paragraph count"),
                            statistic=c("m (standard)", "sd (standard)", "m (delay)", "sd (delay)", "Cohen's d"))))
print(t1)
```


**Table 2**: means, standard deviations, and Cohen's d of lexical diversity and word frequency across the two conditions, Coh-Metrix indices. 
```{r}
t2 = as.table(matrix(c(m_ttr, sd_ttr, m_ttr2, sd_ttr2, effect8, m_mtld, sd_mtld, m_mtld2, sd_mtld2, effect10, m_vocd,sd_vocd,m_vocd2,sd_vocd2,effect9, m_logfreq,sd_logfreq,m_logfreq2, sd_logfreq2, effect11, m_rawfreq,sd_rawfreq, m_rawfreq2,sd_rawfreq2, effect12), byrow=T, ncol=5,
              dimnames=list(measure=c("type-token ratio", "measure of textual lexical diversity","vocd-D", "log frequency all words", "word frequency content words (raw)"),
                            statistic=c("m (standard)", "sd (standard)", "m (delay)", "sd (delay)", "Cohen's d"))))
print(t2)
```


**Table 3**: means, standard deviations, and Cohen's d of lexical diversity and word frequency across the two conditions, TAACO and TAALES indices.
```{r}
t3 = as.table(matrix(c(m_ttr3, sd_ttr3, m_ttr4, sd_ttr4, effect13, m_allsub, sd_allsub, m_allsub2, sd_allsub2,effect14, m_allbnc, sd_allbnc, m_allbnc2,sd_allbnc2, effect15, m_contentsub, sd_contentsub, m_contentsub2, sd_contentsub2, effect16,m_contentbnc, sd_contentbnc, m_contentbnc2, sd_contentbnc2, effect17), byrow=T, ncol=5,
              dimnames=list(measure=c("type-token ratio", "log frequency all words(SUBTLEXus)", "log frequency all words (BNC)", "log frequency content words (SUBTLEXus)", "log frequency content words (BNC)"),
                            statistic=c("m (standard)", "sd (standard)", "m (delay)", "sd (delay)", "Cohen's d"))))
print(t3)
```

####3.1 Descriptive essay indices
In our analyses, we found that the delay condition had a *significant relationship* with transcription fluency, **b = `r round(rs1$coefficients[2,1], 2)`, t(`r rs1$df[2]`) = `r round(rs1$coefficients[2,3], 3)`, p < .001**. Specifically, we found that transcription fluency was greater in the standard condition (M = `r round(m_fluency, 2)`, SD = `r round(sd_fluency,2)`) than in the delay condition (M = `r round(m_fluency2)`, SD = `r round(sd_fluency2, 2)`). Further, we found that there was a *significant relationship* between condition and the number of words written, **b = `r round(rs2$coefficients[2,1], 2)`, t(`r rs2$df[2]`) = `r round(rs2$coefficients[2,3], 3)`, p = `r rs2$coefficients[2,4]`**. Participants in the standard condition used a greater number of words (M = `r round(m_numwords,2)`, SD = `r round(sd_numwords,2)`) than those in the delay condition (M = `r round(m_numwords2,2)`, SD = `r round(sd_numwords2, 2)`). We also found that there was a *significant relationship* between condition and the average word length as measured by letters per word that participants used, **b = `r round(rs3$coefficients[2,1], 2)`, t(`r rs3$df[2]`) = `r round(rs3$coefficients[2,3], 3)`, p = `r rs3$coefficients[2,4]`**. Participants in the standard condition used fewer words (M = `r round(m_lengthl,2)`, SD = `r round(sd_lengthl,2)`) than those in the delay condition (M = `r round(m_lengthl2, 2)`, SD = `r round(sd_lengthl2,2)`). There was also similarly a *significant relationship* between condition and word length as measured by syllables per word that participants used, **b = `r round(rs4$coefficients[2,1], 2)`, t(`r rs4$df[2]`) = `r round(rs4$coefficients[2,3], 3)`, p = `r rs4$coefficients[2,4]`**. Those in the standard condition used words with fewer syllables (M = `r round(m_lengths,2)`, SD = `r round(sd_lengths,2)`) than those in the delay condition (M = `r round(m_lengths2, 2)`, SD = `r round(sd_lengths2, 2)`). No other measures (sentence count, words per sentence, and paragraph count) had a significant relationship with condition (see **Table 1**). Specifically, condition was *not significantly related* to sentence count, **b = `r round(rs5$coefficients[2,1], 2)`, t(`r rs5$df[2]`) = `r round(rs5$coefficients[2,3], 3)`, p = `r rs5$coefficients[2,4]`**, words per sentence, **b = `r round(rs6$coefficients[2,1], 2)`, t(`r rs6$df[2]`) = `r round(rs6$coefficients[2,3], 3)`, p = `r rs6$coefficients[2,4]`**, or paragraph count, **b = `r round(rs7$coefficients[2,1], 2)`, t(`r rs7$df[2]`) = `r round(rs7$coefficients[2,3], 3)`, p = `r rs7$coefficients[2,4]`**.


####3.2 Lexical sophistication 
When we examined lexical sophistication, we found that there was a *significant relationship* between condition and TTR, **b = `r round(rs8$coefficients[2,1], 2)`, t(`r rs8$df[2]`) = `r round(rs8$coefficients[2,3], 3)`, p = `r rs8$coefficients[2,4]`**. That is, those in the standard condition had a lower TTR (M = `r round(m_ttr,2)`, SD = `r round(sd_ttr,2)`) than those in the delay condition (M = `r round(m_ttr2, 2)`, SD = `r round(sd_ttr2,2)`). We also found a *marginally significant* relationship between vocd-D and condition, **b = `r round(rs9$coefficients[2,1], 2)`, t(`r rs3$df[2]`) = `r round(rs9$coefficients[2,3], 3)`, p = `r rs9$coefficients[2,4]`**, such that those in the standard condition had lower vocd-D (M = `r round(m_vocd,2)`, SD = `r round(sd_vocd,2)`) than those in the delay condition (M = `r round(m_vocd2,2)`, SD = `r round(sd_vocd2,2)`). There was *no significant relationship* between the measure of textual lexical diversity and condition, **b = `r round(rs10$coefficients[2,1], 2)`, t(`r rs10$df[2]`) = `r round(rs10$coefficients[2,3], 3)`, p = `r rs10$coefficients[2,4]`**. However, there was a *significant relationship* between the log frequency of all words and condition, **b = `r round(rs11$coefficients[2,1], 2)`, t(`r rs11$df[2]`) = `r round(rs11$coefficients[2,3], 3)`, p = `r rs11$coefficients[2,4]`**. such that those in the standard condition had a higher log frequency of words (M = `r round(m_logfreq,2)`, SD = `r round(sd_logfreq,2)`) than those in the delay condition (M = `r round(m_logfreq2, 2)`, SD = `r round(sd_logfreq2,2)`). And there was also a *significant relationship* between the raw frequency of content words and condition, **b = `r round(rs12$coefficients[2,1], 2)`, t(`r rs12$df[2]`) = `r round(rs12$coefficients[2,3], 3)`, p = `r rs12$coefficients[2,4]`**. Those in the standard condition had a higher raw frequency of content words (M = `r round(m_rawfreq,2)`, SD = `r round(sd_rawfreq,2)`) than those in the delay condition (M = `r round(m_rawfreq2,2)`, SD = `r round(sd_rawfreq2,2)`) (see **Table 2**). 

We find similar results when examining lexical diersity and word frequency using TAACO and TAALES (see **Table 3**). We found that there was a *significant relationship* between TTR and condition, **b = `r round(rs13$coefficients[2,1], 2)`, t(`r rs13$df[2]`) = `r round(rs13$coefficients[2,3], 3)`, p = `r rs13$coefficients[2,4]`**, such that those in the standard condition had a lower TTR (M = `r round(m_ttr3,2)`, SD = `r round(sd_ttr3,2)`) than those in the delay condition (M = `r round(m_ttr4,2)`, SD = `r round(sd_ttr4,2)`). There was also a *significant relationship* between the frequency of all words by condition using SUBTLEXus, **b = `r round(rs14$coefficients[2,1], 2)`, t(`r rs14$df[2]`) = `r round(rs14$coefficients[2,3], 3)`, p = `r rs14$coefficients[2,4]`**, such that participants in the standard condition had higher log frequencies (M = `r round(m_allsub,2)`, SD = `r round(sd_allsub,2)`) than those in the delay condition (M = `r round(m_allsub2,2)`, SD = `r round(sd_allsub2,2)`), and using BNC, **b = `r round(rs15$coefficients[2,1], 2)`, t(`r rs15$df[2]`) = `r round(rs15$coefficients[2,3], 3)`, p = `r rs15$coefficients[2,4]`**, such that those in the standard condition had higher log frequencies (M = `r round(m_allbnc,2)`, SD = `r round(sd_allbnc,2)`) than those in the delay condition (M = `r round(m_allbnc2,2)`, SD = `r round(sd_allbnc2,2)`). Finally, there was also a *significant relationship* between the log frequency of content words by condition using SUBTLEXus, **b = `r round(rs16$coefficients[2,1], 2)`, t(`r rs16$df[2]`) = `r round(rs16$coefficients[2,3], 3)`, p = `r rs16$coefficients[2,4]`**, such that those in the standard condition had higher log frequencies (M = `r round(m_contentsub,2)`, SD = `r round(sd_contentsub,2)`) than those in the delay condition (M = `r round(m_contentsub2,2)`, SD = `r round(sd_contentsub2,2)`), and using BNC, **b = `r round(rs17$coefficients[2,1], 2)`, t(`r rs17$df[2]`) = `r round(rs17$coefficients[2,3], 3)`, p = `r rs17$coefficients[2,4]`**, such that those in the standard condition had higer log frequencies (M = `r round(m_contentbnc,2)`, SD = `r round(sd_contentbnc,2)`) than those in the delay condition, (M = `r round(m_contentbnc2,2)`, SD = `r round(sd_contentbnc2,2)`).

####Comparison to Medimorec et al. results
Overall, we were able to reproduce the general results produced by Medimorec et al. (2017). Conceptually we find support for Medimorec et al.'s claim that essays written in a less fluent way are more lexically diverse and use less frequent words. However, when we make a closer comparison to Medimorec et al.'s exact reported results, we find that we have failed to make a successful reproduction. There were a number of minor numerical errors that could have been reporting error due to rounding errors and the like, and there was one major numerical error. The mean squared error of our analysis of the effect of condition on the log frequency of all words was far smaller than what Medimorec et al. reported. Further, for nine of the analyses that Medimorec et al. presumably carried out were not fully reported with all accompanying statistics. This insufficient information made it impossible for us to truly determine if our analyses reproduced theirs. Therefore, we consider this replication a failure.



```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
